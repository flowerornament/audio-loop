---
phase: 03-iteration-tools
plan: 02
type: execute
---

<objective>
Add audioloop compare command for side-by-side feature comparison.

Purpose: Help Claude understand what changed between iterations by showing specific, interpretable deltas.
Output: Working `audioloop compare <a.wav> <b.wav>` command with direction indicators, significance flags, and interpretive context.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-iteration-tools/RESEARCH.md
@.planning/phases/02-analysis-core/02-02-SUMMARY.md
@src/audioloop/cli.py
@src/audioloop/analyze.py
@src/audioloop/interpret.py

**From Phase 2:**
- analyze() returns AnalysisResult with .to_dict() method
- interpret.py has format_analysis_human() and reference range functions
- CLI pattern: human-readable default, `--json` flag for JSON output
- Exit codes: 0=success, 1=command error, 2=system error

**From RESEARCH.md:**
- Reuse analyze() twice, compute deltas
- Significance threshold: >10% change
- Direction indicators: up/down/unchanged
- Interpretations: centroid_down = "darker/warmer", attack_down = "snappier", etc.

**Tech available:**
- typer, rich, librosa, numpy (from Phases 1-2)

**Constraining decisions:**
- [Phase 2] Reference ranges, not judgments - context-dependent interpretation
- [Phase 2] Exit code consistency: 0/1/2 pattern
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create compare module with delta computation</name>
  <files>src/audioloop/compare.py</files>
  <action>
Create src/audioloop/compare.py with:

1. ComparisonResult dataclass:
   - file_a, file_b: str
   - duration_a, duration_b: float
   - deltas: dict[str, FeatureDelta]
   - summary: dict with significant_changes list and overall interpretation

2. FeatureDelta dataclass:
   - metric: str (e.g., "spectral.left.centroid_hz")
   - value_a, value_b: float
   - delta: float (b - a)
   - percent_change: float | None (handle division by zero)
   - direction: Literal["up", "down", "unchanged"]
   - significant: bool (abs(percent_change) > 10)
   - unit: str
   - interpretation: str | None (from INTERPRETATIONS dict)

3. compare_audio(file_a: Path, file_b: Path) -> ComparisonResult:
   - Call analyze() on both files
   - Flatten nested dicts (spectral.left.centroid_hz, etc.)
   - Compute deltas for all numeric fields
   - Add interpretations for significant changes
   - Build summary with list of significant changes

4. INTERPRETATIONS dict (from RESEARCH.md):
   - centroid_hz: down="darker/warmer", up="brighter"
   - attack_ms: down="snappier attack", up="slower attack"
   - rms: down="quieter", up="louder"
   - width: down="narrower", up="wider"
   - crest_factor: down="more compressed", up="more dynamic"

5. format_comparison_human(result: ComparisonResult) -> str:
   - Group by category (spectral, temporal, stereo, loudness)
   - Show only significant changes prominently
   - Include direction arrows (↑/↓)
   - Add interpretive context

Note: Per-channel deltas (spectral.left vs spectral.right) are computed separately.
  </action>
  <verify>
Create quick test:
```python
from audioloop.compare import compare_audio
from pathlib import Path
result = compare_audio(Path("tests/fixtures/test_tone.wav"), Path("tests/fixtures/test_tone.wav"))
# Same file = all deltas should be zero/unchanged
```
  </verify>
  <done>
- compare_audio() returns ComparisonResult with all deltas
- Deltas include direction, significance, and interpretation
- format_comparison_human() produces readable output
  </done>
</task>

<task type="auto">
  <name>Task 2: Add compare command to CLI with tests</name>
  <files>src/audioloop/cli.py, tests/test_compare.py</files>
  <action>
Add compare command to cli.py:
- `audioloop compare <file_a> <file_b>`
- `--json` flag for JSON output
- Validate both files exist (exit 2 if not)
- Call compare_audio(), catch AnalysisError (exit 1)
- Human-readable output shows significant changes with interpretation
- JSON output includes full deltas structure

Create tests/test_compare.py:
1. test_compare_identical_files - same file, all deltas zero
2. test_compare_cli_human_output - verify format with significant changes
3. test_compare_cli_json_output - verify JSON structure
4. test_compare_file_not_found - verify exit code 2
5. test_direction_and_significance - verify up/down/unchanged logic

Use tests/fixtures/test_tone.wav as baseline. Create a second fixture or mock different analysis results for delta testing.
  </action>
  <verify>
Run: audioloop compare tests/fixtures/test_tone.wav tests/fixtures/test_tone.wav
Expected: Shows comparison with all "unchanged" (same file)

Run: audioloop compare tests/fixtures/test_tone.wav tests/fixtures/test_tone.wav --json
Expected: JSON output with deltas

Run: python -m pytest tests/test_compare.py -v
Expected: All tests pass
  </verify>
  <done>
- `audioloop compare` command works end-to-end
- Human output shows significant changes with interpretations
- JSON output provides full delta structure for Claude
- Tests cover identical files, CLI output, and error handling
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `audioloop compare file1.wav file2.wav` produces human-readable output
- [ ] `audioloop compare file1.wav file2.wav --json` produces JSON
- [ ] Significant changes (>10%) are highlighted
- [ ] Interpretive text appears (darker/warmer, snappier, etc.)
- [ ] `python -m pytest tests/test_compare.py -v` passes
- [ ] `audioloop --help` shows compare command
</verification>

<success_criteria>
- audioloop compare command works end-to-end
- Output includes direction (up/down), significance flags, interpretations
- JSON output structured for Claude parsing
- Exit codes follow established pattern (0/1/2)
- All tests pass
- Phase 3 complete after this plan
</success_criteria>

<output>
After completion, create `.planning/phases/03-iteration-tools/03-02-SUMMARY.md`:

Include:
- Summary of Phase 3 completion
- Both commands (play, compare) working
- Milestone 1 (Working Feedback Loop) complete status
</output>

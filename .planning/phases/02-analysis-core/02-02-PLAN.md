---
phase: 02-analysis-core
plan: 02
type: execute
domain: cli
---

<objective>
Add `audioloop analyze` CLI command with JSON and human-readable output formats.

Purpose: Expose the analysis engine to Claude and users via the CLI.
Output: Working `audioloop analyze <file.wav>` command with interpretive summaries.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-analysis-core/RESEARCH.md

# Prior plan in this phase:
@.planning/phases/02-analysis-core/02-01-SUMMARY.md

# Existing source files:
@src/audioloop/cli.py
@src/audioloop/analyze.py

**Tech stack available:** typer, rich, pytest, librosa, soundfile, pyloudnorm
**Established patterns:** CLI with typer, JSON output with --json flag
**Constraining decisions:**
- Phase 1: JSON output format for Claude parsing
- Phase 1: Human-readable output with rich formatting
- RESEARCH.md: No hardcoded thresholds - provide reference ranges instead
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create interpretation layer</name>
  <files>src/audioloop/interpret.py</files>
  <action>
Create interpret.py with functions to add context to raw values:

1. **interpret_centroid(hz: float) -> str**
   Return reference context like:
   - "very dark (<300Hz)" / "dark/warm (300-800Hz)" / "neutral (800-2000Hz)" / "bright (2000-4000Hz)" / "very bright (>4000Hz)"
   Format: "{value:.0f} Hz ({description})"

2. **interpret_crest_factor(cf: float) -> str**
   Return dynamics context:
   - "very compressed (<3)" / "moderate dynamics (3-10)" / "punchy/dynamic (10-20)" / "very dynamic (>20)"

3. **interpret_stereo_width(width: float) -> str**
   Return width context:
   - "mono (<0.1)" / "narrow (0.1-0.3)" / "moderate (0.3-0.6)" / "wide (0.6-0.8)" / "very wide (>0.8)"

4. **interpret_loudness(lufs: float) -> str**
   Return loudness context:
   - Reference: Streaming targets -14 LUFS, broadcast -24 LUFS
   - Format: "{value:.1f} LUFS ({context})"

5. **format_analysis_human(result: AnalysisResult) -> str**
   Build a rich-formatted string with sections:
   - FILE INFO: filename, duration, sample rate, channels
   - SPECTRAL (L/R columns if stereo): centroid, rolloff, flatness, bandwidth with interpretations
   - DYNAMICS: RMS, crest factor, attack time
   - STEREO: width, correlation
   - LOUDNESS: LUFS with context

Use rich Tables for aligned output. Keep it concise - Claude reads this too.

Do NOT hardcode "warm" or "bright" as judgments - these are reference ranges only. Let Claude and users interpret based on context.
  </action>
  <verify>`python -c "from audioloop.interpret import format_analysis_human"` imports successfully</verify>
  <done>Interpretation functions work, format_analysis_human produces readable output</done>
</task>

<task type="auto">
  <name>Task 2: Add analyze command to CLI</name>
  <files>src/audioloop/cli.py</files>
  <action>
Add analyze command to cli.py:

```python
@app.command()
def analyze(
    file: Path = typer.Argument(..., help="WAV file to analyze"),
    json_output: bool = typer.Option(False, "--json", "-j", help="Output as JSON"),
) -> None:
```

Implementation:
1. Validate file exists and is readable
2. Call analyze.analyze(file)
3. If --json: Output AnalysisResult as JSON (use dataclasses.asdict + json.dumps with indent=2)
4. Else: Output format_analysis_human(result) via rich console

Error handling:
- File not found: Print error, exit 2 (system error)
- Analysis error: Print error, exit 1

Match the exit code pattern from render command (0=success, 1=analysis error, 2=system error).

Import the analyze function and interpret module at top of file.
  </action>
  <verify>`audioloop analyze --help` shows the command, `audioloop analyze tests/fixtures/test_tone.wav` produces output</verify>
  <done>analyze command works with both --json and human-readable modes</done>
</task>

<task type="auto">
  <name>Task 3: Add CLI integration tests</name>
  <files>tests/test_analyze_cli.py</files>
  <action>
Create tests/test_analyze_cli.py with:

1. **test_analyze_json_output**: Run `audioloop analyze <wav> --json`, verify valid JSON with expected keys
2. **test_analyze_human_output**: Run `audioloop analyze <wav>`, verify output contains expected sections (SPECTRAL, DYNAMICS, etc.)
3. **test_analyze_file_not_found**: Run with nonexistent file, verify exit code 2
4. **test_analyze_json_schema**: Verify JSON output matches PROJECT.md schema structure

Use subprocess.run to invoke the CLI (like Phase 1 integration tests).
Use the test_tone.wav fixture from 02-01.

Keep tests fast and independent.
  </action>
  <verify>pytest tests/test_analyze_cli.py -v passes all tests</verify>
  <done>4+ CLI integration tests passing</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `audioloop analyze tests/fixtures/test_tone.wav` produces human-readable output
- [ ] `audioloop analyze tests/fixtures/test_tone.wav --json` produces valid JSON
- [ ] JSON output matches PROJECT.md schema structure
- [ ] `audioloop analyze nonexistent.wav` exits with code 2
- [ ] `pytest tests/test_analyze_cli.py -v` passes all tests
- [ ] All Phase 2 tests pass: `pytest tests/test_analyze*.py -v`
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- `audioloop analyze` command works end-to-end
- JSON output is parseable by Claude
- Human output provides useful context without hardcoded judgments
- Phase 2 complete, ready for Phase 3 (Iteration Tools)
</success_criteria>

<output>
After completion, create `.planning/phases/02-analysis-core/02-02-SUMMARY.md` following the summary template.

This completes Phase 2. Update STATE.md:
- Phase 2 status: complete
- Ready for Phase 3
</output>
